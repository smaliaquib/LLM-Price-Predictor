{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6c22c7a-7bde-45fc-b4e5-76ce49744377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b034dd-daa9-4193-8323-1a7f4bba447e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "MODEL = \"gpt-4o-mini\"\n",
    "openai = OpenAI()\n",
    "\n",
    "MODEL_LLAMA = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8783ec2a-02b1-46d1-b591-54ec88e92366",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an helpful assistant known as FlightAI\"\n",
    "system_prompt += \"reply no more than 1 senetnce\"\n",
    "system_prompt += \"Be accurate, if don't know the answer just say so\"\n",
    "\n",
    "ticket_prices = {\"london\": \"$799\", \"paris\": \"$899\", \"tokyo\": \"$1400\", \"berlin\": \"$499\"}\n",
    "\n",
    "def get_ticket_price(destination_city):\n",
    "    print(f\"Tool get_ticket_price called for {destination_city}\")\n",
    "    city = destination_city.lower()\n",
    "    return ticket_prices.get(city, \"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2773faa4-b866-4d9b-b32f-d810171d317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_function = {\n",
    "    \"name\": \"get_ticket_price\",\n",
    "    \"description\": \"Get the price of a return ticket to the destination city. Call this whenever you need to know the ticket price, for example when a customer asks 'How much is a ticket to this city'\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"destination_city\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city that the customer wants to travel to\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"destination_city\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c52b1cf-9c45-4e9e-abe7-7f2bb2212954",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\"type\": \"function\", \"function\": price_function}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5c23c4d-58a0-4e63-a500-2058c3daa544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history, model='llama'):\n",
    "    if model == \"openai\":\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "        response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)\n",
    "    \n",
    "        if response.choices[0].finish_reason==\"tool_calls\":\n",
    "            message = response.choices[0].message\n",
    "            response, city = handle_tool_call(message)\n",
    "            messages.append(message)\n",
    "            messages.append(response)\n",
    "            response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "        return response.choices[0].message.content\n",
    "    elif model == \"llama\":\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "        response = ollama.chat(model=MODEL_LLAMA, messages=messages, tools=tools)\n",
    "        if 'message' in response and isinstance(response['message'], dict) and 'content' in response['message']:\n",
    "            content = response['message']['content']\n",
    "            if isinstance(content, dict) and 'finish_reason' in content:\n",
    "                if content['finish_reason'] == \"tool_calls\":\n",
    "                    message = response['message']['content']\n",
    "                    response, city = handle_tool_call(message)\n",
    "                    messages.append(message)\n",
    "                    messages.append(response)\n",
    "                    response = ollama.chat(model=MODEL_LLAMA, messages=messages)\n",
    "                return  response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf9efbb3-b68a-4a78-ac74-c9fcb69312f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_tool_call(message):\n",
    "    tool_call = message.tool_calls[0]\n",
    "    arguments = json.loads(tool_call.function.arguments)\n",
    "    city = arguments.get('destination_city')\n",
    "    price = get_ticket_price(city)\n",
    "    response = {\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": json.dumps({\"destination_city\": city,\"price\": price}),\n",
    "        \"tool_call_id\": tool_call.id\n",
    "    }\n",
    "    return response, city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8126bcaf-d103-4e9b-b6fc-51a62ada651e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7889\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7889/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Aquib\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Aquib\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Aquib\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2088, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Aquib\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1633, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Aquib\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 850, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Aquib\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\chat_interface.py\", line 869, in _submit_fn\n",
      "    history = self._append_message_to_history(response, history, \"assistant\")\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Aquib\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\chat_interface.py\", line 800, in _append_message_to_history\n",
      "    message_dicts = self._message_as_message_dict(message, role)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Aquib\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\chat_interface.py\", line 838, in _message_as_message_dict\n",
      "    for x in msg.get(\"files\", []):\n",
      "             ^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'get'\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeaf81f-d26b-42f5-b3f4-a166a6c32644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
