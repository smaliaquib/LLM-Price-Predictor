{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5c60d2-1b4e-4226-b683-361f1d5804d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "import gradio as gr\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import subprocess\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "315ae4b2-1f82-40c5-a429-237e70da6868",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "MODEL_LLAMA = \"llama3.2\"\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47aa79c2-1712-4740-9206-670ee4aad16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a helpful assistant who can generate dataset if given business problem.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad8c7f53-e966-4bb7-8347-6fdbabb7e9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prompt_tabular(business_problem, dataset_format, file_format, num_samples):\n",
    "\n",
    "    user_message = f\"\"\"\n",
    "    The business problem is: {business_problem}. \\n\n",
    "    The dataset is expected to be in {dataset_format}.\n",
    "    For the dataset types such as tabular or time series implement python code for creating the dataset.\n",
    "    If the generated dataset contains several entities, i.e. products, users write the output for these entities into separate files.\n",
    "    The dependencies for python code should include only standard python libraries such as numpy, pandas and built-in-libraries.\n",
    "    The output dataset is stored as a {file_format} file and contains {num_samples} samples.\\n\n",
    "    \"\"\"\n",
    "    return user_message\n",
    "\n",
    "\n",
    "def get_user_prompt_text(business_problem, dataset_format, file_format):\n",
    "\n",
    "    user_message = f\"\"\"\n",
    "    The business problem is: {business_problem}. \\n\n",
    "    The dataset is expected to be in {dataset_format}.\n",
    "    For the text type return the generated dataset and the python code to write the output to the files.\n",
    "    If the generated dataset contains several entities, i.e. products, users write the output for these entities into separate files.\n",
    "    The dependencies for python code should include only standard python libraries such as numpy, pandas and built-in-libraries.\n",
    "    The output dataset is stored as a {file_format}\n",
    "    \"\"\"\n",
    "    return user_message\n",
    "\n",
    "def select_user_prompt(business_problem, dataset_format, file_format, num_samples):\n",
    "    user_prompt = \"\"\n",
    "    if dataset_format == \"Text\":\n",
    "        user_prompt = get_user_prompt_text(business_problem, dataset_format, file_format)\n",
    "    elif dataset_format in [\"Tabular\", \"Time-series\"]:\n",
    "        user_prompt = get_user_prompt_tabular(business_problem, dataset_format, file_format, num_samples)\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b542fd71-2626-4227-82bc-f096e3b13ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(business_problem, dataset_format, file_format, num_samples):\n",
    "\n",
    "    user_prompt = select_user_prompt(\n",
    "                    business_problem, dataset_format, file_format, num_samples\n",
    "                )\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        yield response\n",
    "    return response\n",
    "\n",
    "\n",
    "def stream_llama(business_problem, dataset_format, file_format, num_samples):\n",
    "    user_prompt = select_user_prompt(\n",
    "                    business_problem, dataset_format, file_format, num_samples\n",
    "                )\n",
    "    message = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "      ]\n",
    "    stream = ollama.chat(model=MODEL_LLAMA, messages=message, stream=True)\n",
    "    \n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk['message']['content']\n",
    "        yield response\n",
    "    return response\n",
    "\n",
    "def generate_dataset(business_problem, dataset_format, file_format, num_samples, model):\n",
    "    if model == \"GPT\":\n",
    "        result = stream_gpt(business_problem, dataset_format, file_format, num_samples)\n",
    "    elif model == \"Llama\":\n",
    "        result = stream_llama(business_problem, dataset_format, file_format, num_samples)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    for stream_so_far in result:\n",
    "        yield stream_so_far\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c726b22-1101-438e-9efe-209a812177ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Hello from Conda virtual environment!\n",
      "\n",
      "Errors: \n"
     ]
    }
   ],
   "source": [
    "def extract_code(text):\n",
    "    \"\"\"Extract Python code from a markdown-style code block.\"\"\"\n",
    "    match = re.search(r\"```python(.*?)```\", text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "def get_conda_python():\n",
    "    \"\"\"Get the Python interpreter path in the active Conda environment.\"\"\"\n",
    "    conda_prefix = os.environ.get(\"CONDA_PREFIX\")\n",
    "    if not conda_prefix:\n",
    "        raise EnvironmentError(\"No active Conda environment found.\")\n",
    "    return os.path.join(conda_prefix, \"bin\", \"python\")\n",
    "\n",
    "def execute_code_in_conda_env(text):\n",
    "    \"\"\"Execute extracted Python code inside the active Conda environment.\"\"\"\n",
    "    # python_interpreter = get_conda_python()\n",
    "    python_interpreter = \"C:\\\\Users\\\\Aquib\\\\anaconda3\\\\envs\\\\llms\\\\python.exe\"\n",
    "    \n",
    "    if not os.path.exists(python_interpreter):\n",
    "        raise EnvironmentError(\"Conda Python interpreter not found.\")\n",
    "\n",
    "    code_str = extract_code(text)\n",
    "    if not code_str:\n",
    "        print(\"No valid Python code found in the input text.\")\n",
    "        return\n",
    "    \n",
    "    command = [python_interpreter, \"-c\", code_str]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(\"Output:\", result.stdout)\n",
    "        print(\"Errors:\", result.stderr)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error while executing code: {e}\")\n",
    "\n",
    "# Example usage\n",
    "code_string = \"\"\"```python\n",
    "print('Hello from Conda virtual environment!')\n",
    "```\"\"\"\n",
    "\n",
    "execute_code_in_conda_env(code_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ce6842e-fc02-46ad-ae24-e8233b36adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as ui:\n",
    "    gr.Markdown(\"## Create a dataset for a business problem\")\n",
    "    with gr.Column():\n",
    "        business_problem = gr.Textbox(label=\"Business problem\", lines=2)\n",
    "        dataset_type = gr.Dropdown(\n",
    "            [\"Tabular\", \"Time-series\", \"Text\"], label=\"Dataset modality\"\n",
    "        )\n",
    "        dataset_format = gr.Dropdown([\"JSON\", \"csv\", \"parquet\", \"Markdown\"], label=\"Output format\")\n",
    "        num_samples = gr.Number(label=\"Number of samples (for tabular and time-series data)\", value=10, precision=0)\n",
    "        model = gr.Dropdown([\"GPT\", \"Llama\"], label=\"Select model\", value=\"GPT\")\n",
    "    with gr.Row():\n",
    "        dataset_run = gr.Button(\"Create a dataset\")\n",
    "        code_run = gr.Button(\"Execute code for a dataset\")\n",
    "    with gr.Row():\n",
    "        dataset_out = gr.Textbox(label=\"Generated Dataset\")\n",
    "        code_out = gr.Textbox(label=\"Executed code\")\n",
    "    dataset_run.click(\n",
    "        generate_dataset,\n",
    "        inputs=[business_problem, dataset_type, dataset_format, num_samples, model],\n",
    "        outputs=[dataset_out]\n",
    "    )\n",
    "    code_run.click(execute_code_in_conda_env, inputs=[dataset_out], outputs=[code_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cd7179d-fd71-480c-a4ce-50ad36240a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1137be-ca50-44d2-b09f-d07587bc43e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
